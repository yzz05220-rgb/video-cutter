#!/usr/bin/env python3
"""
å®Œæ•´åˆ†æå™¨ - æ•´åˆåŸæœ‰è§„åˆ™ + LLMæ™ºèƒ½åˆ†æ
1. ä½¿ç”¨config.yamlçš„å®Œæ•´è¯­æ°”è¯åˆ—è¡¨
2. LLMè¯†åˆ«é¢†åŸŸå’Œé”™åˆ«å­—
3. æ™ºèƒ½ä¸Šä¸‹æ–‡åˆ¤æ–­
"""

import json
import sys
import yaml
import re
from pathlib import Path

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def load_config(config_file='config.yaml'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    script_dir = Path(__file__).parent
    config_path = script_dir / config_file

    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def analyze_domain_and_typos(text):
    """
    LLMåˆ†æï¼šè¯†åˆ«é¢†åŸŸå’Œé”™åˆ«å­—
    è¿”å›: (domain, typos_dict)
    """
    print("[LLMåˆ†æ] æ­£åœ¨è¯†åˆ«å†…å®¹é¢†åŸŸå’Œé”™åˆ«å­—...")

    # æç¤ºè¯
    prompt = f"""è¯·åˆ†æä»¥ä¸‹å£æ’­æ–‡ç¨¿ï¼Œå®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š

**ä»»åŠ¡1ï¼šè¯†åˆ«å†…å®¹é¢†åŸŸ**
åˆ¤æ–­è¿™æ˜¯å±äºå“ªä¸ªé¢†åŸŸçš„è§†é¢‘ï¼ˆå¦‚ï¼šçŸ¥è¯†åˆ†äº«ã€ç§‘æŠ€è¯„æµ‹ã€ç”Ÿæ´»vlogã€æ¸¸æˆè§£è¯´ã€æ•™è‚²è¯¾ç¨‹ç­‰ï¼‰

**ä»»åŠ¡2ï¼šè¯†åˆ«é”™åˆ«å­—å’ŒåŒéŸ³å­—é”™è¯¯**
åˆ—å‡ºæ–‡ç¨¿ä¸­å¯èƒ½çš„é”™åˆ«å­—æˆ–åŒéŸ³å­—é”™è¯¯ï¼Œå¹¶ç»™å‡ºæ­£ç¡®çš„å†™æ³•ã€‚

**åŸæ–‡ç¨¿**ï¼ˆå‰1000å­—ï¼‰ï¼š
{text[:1000]}
...

**è¿”å›æ ¼å¼**ï¼ˆJSONï¼‰ï¼š
{{
  "domain": "è§†é¢‘é¢†åŸŸ",
  "typos": [
    {{"wrong": "é”™è¯¯å†™æ³•", "right": "æ­£ç¡®å†™æ³•", "position": "ä¸Šä¸‹æ–‡æç¤º"}},
    ...
  ]
}}

å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„é”™åˆ«å­—ï¼Œtyposè¿”å›ç©ºåˆ—è¡¨[]ã€‚
"""

    # åœ¨Skillsç¯å¢ƒä¸­ï¼Œè®©å½“å‰LLMåˆ†æ
    print("æç¤ºè¯å·²ç”Ÿæˆï¼Œè¯·å°†ä¸Šè¿°å†…å®¹å‘é€ç»™LLMè¿›è¡Œåˆ†æ")
    print("ï¼ˆåœ¨Skillsç¯å¢ƒä¸­ï¼Œè¿™åº”è¯¥è‡ªåŠ¨å®Œæˆï¼‰")
    print()

    # è¿”å›é»˜è®¤å€¼ï¼ˆå®é™…åº”è¯¥ç”±LLMè¿”å›ï¼‰
    return "çŸ¥è¯†åˆ†äº«", {}

def is_filler_by_context(char, before_text, after_text, config):
    """
    åŸºäºä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦ä¸ºè¯­æ°”è¯
    ä½¿ç”¨config.yamlä¸­çš„è§„åˆ™
    """
    # è·å–é…ç½®çš„è¯­æ°”è¯åˆ—è¡¨
    filler_words = config.get('filler_words', [])

    # å¦‚æœä¸åœ¨åˆ—è¡¨ä¸­ï¼Œç›´æ¥ä¿ç•™
    if char not in filler_words:
        return False, "ä¸åœ¨è¯­æ°”è¯åˆ—è¡¨"

    # è·å–é«˜çº§è‡ªå®šä¹‰è§„åˆ™
    custom_rules = config.get('advanced', {}).get('custom_rules', [])

    # æ£€æŸ¥è‡ªå®šä¹‰è§„åˆ™
    for rule in custom_rules:
        if rule.get('regex'):
            pattern = rule.get('pattern', '')
            if re.search(pattern, before_text + char + after_text):
                return True, f"è‡ªå®šä¹‰è§„åˆ™: {rule.get('name')}"

    # ä¸Šä¸‹æ–‡æ™ºèƒ½åˆ¤æ–­ï¼ˆä½¿ç”¨åŸç‰ˆlogicï¼‰
    # ç‰¹æ®Šå¤„ç†"å•Š"
    if char == 'å•Š':
        # ä¿ç•™ï¼šå¥æœ«çš„"å•Š"
        if len(after_text) > 0 and after_text[0] in 'ã€‚ï¼ï¼Ÿ':
            return False, "å¥æœ«è¯­æ°”åŠ©è¯"

        # ä¿ç•™ï¼šåˆ—ä¸¾ä¸­çš„"å•Š"
        if 'ï¼Œå•Š' in before_text[-5:] or 'ã€å•Š' in before_text[-5:]:
            return False, "åˆ—ä¸¾è¯­æ°”è¯"

        # ä¿ç•™ï¼šå¼ºè°ƒè¯­æ°”çš„"å•Š"
        if any(p in before_text[-10:] for p in ['çš„è¯´å•Š', 'å¯¹çš„å•Š', 'æ˜¯çš„å•Š', 'æ˜¯å•Š']):
            return False, "å¼ºè°ƒè¯­æ°”"

        # åˆ é™¤ï¼šå¥é¦–çš„"å•Š"
        if len(before_text) > 0 and before_text[-1] in 'ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š\n':
            return True, "å¥é¦–çŠ¹è±«è¯"

        # åˆ é™¤ï¼šé‡å¤çš„"å•Š"
        if 'å•Š' in before_text[-3:]:
            return True, "é‡å¤è¯­æ°”è¯"

    # ç‰¹æ®Šå¤„ç†"å‘ƒ"ã€"å—¯" - å‡ ä¹æ€»æ˜¯åˆ é™¤
    if char in ['å‘ƒ', 'å—¯']:
        # æ£€æŸ¥æ˜¯å¦åœ¨å¥é¦–æˆ–å¥ä¸­
        if len(before_text) == 0 or before_text[-1] in 'ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š\n':
            return True, "æ€è€ƒåœé¡¿è¯"
        return True, "æ€è€ƒçŠ¹è±«è¯"

    # å…¶ä»–è¯­æ°”è¯ï¼ˆé‚£ä¸ªã€ç„¶åã€å°±æ˜¯ï¼‰
    # æ£€æŸ¥æ˜¯å¦é‡å¤å‡ºç°
    if char in ['é‚£ä¸ª', 'ç„¶å', 'å°±æ˜¯']:
        # æ£€æŸ¥å‰10ä¸ªå­—ç¬¦å†…æ˜¯å¦å‡ºç°è¿‡
        if char in before_text[-10:]:
            return True, f"é‡å¤çš„'{char}'"

    # é»˜è®¤åˆ é™¤ï¼ˆå› ä¸ºåœ¨åˆ—è¡¨ä¸­ï¼‰
    return True, f"è¯­æ°”è¯'{char}'"

def analyze_transcript(transcript_file, output_filter_file, config_file='config.yaml', use_llm=True):
    """å®Œæ•´åˆ†ææµç¨‹"""

    # åŠ è½½é…ç½®
    config = load_config(config_file)

    with open(transcript_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    segments = data['segments']
    text = ''.join([s['char'] for s in segments])

    print("=" * 60)
    print("ğŸ¬ å®Œæ•´æ™ºèƒ½åˆ†æå™¨")
    print("=" * 60)
    print()

    # 1. LLMåˆ†æï¼šé¢†åŸŸè¯†åˆ«å’Œé”™åˆ«å­—
    if use_llm:
        domain, typos = analyze_domain_and_typos(text)
        print(f"ğŸ“š å†…å®¹é¢†åŸŸ: {domain}")
        print(f"ğŸ” å‘ç°é”™åˆ«å­—: {len(typos)} ä¸ª")

        if typos:
            for typo in typos[:5]:
                print(f"  - \"{typo['wrong']}\" â†’ \"{typo['right']}\" ({typo.get('position', '')})")
        print()

    # 2. æ£€æµ‹è¯­æ°”è¯ï¼ˆä½¿ç”¨config.yamlçš„å®Œæ•´åˆ—è¡¨ï¼‰
    print("[2/3] æ£€æµ‹è¯­æ°”è¯ï¼ˆä½¿ç”¨config.yamlè§„åˆ™ï¼‰...")

    filler_words = config.get('filler_words', [])
    print(f"  é…ç½®çš„è¯­æ°”è¯åˆ—è¡¨: {len(filler_words)} ä¸ª")

    potential_fillers = []
    for i, seg in enumerate(segments):
        if seg['char'] in filler_words:
            # è·å–ä¸Šä¸‹æ–‡
            start_idx = max(0, i - 10)
            end_idx = min(len(segments), i + 11)
            context = ''.join([s['char'] for s in segments[start_idx:end_idx]])
            context_index = i - start_idx

            # è·å–å‰åæ–‡æœ¬
            before_text = context[:context_index]
            after_text = context[context_index + 1:]

            # ä¸Šä¸‹æ–‡åˆ¤æ–­
            should_delete, reason = is_filler_by_context(
                seg['char'],
                before_text,
                after_text,
                config
            )

            potential_fillers.append({
                'index': i,
                'char': seg['char'],
                'start_ms': seg['start'],
                'end_ms': seg['end'],
                'context': context,
                'should_delete': should_delete,
                'reason': reason
            })

    print(f"  å‘ç°æ½œåœ¨è¯­æ°”è¯: {len(potential_fillers)} ä¸ª")

    deleted_count = sum(1 for f in potential_fillers if f['should_delete'])
    kept_count = sum(1 for f in potential_fillers if not f['should_delete'])

    print(f"  åˆ é™¤è¯­æ°”è¯: {deleted_count} ä¸ª")
    print(f"  ä¿ç•™è¯­æ°”è¯: {kept_count} ä¸ª")
    print()

    # 3. æ£€æµ‹é‡å¤å­—
    print("[3/3] æ£€æµ‹é‡å¤å­—...")
    repeat_count = 0
    repeat_deletions = []

    for i in range(len(segments) - 1):
        if segments[i]['char'] == segments[i+1]['char']:
            repeat_count += 1
            repeat_deletions.append((segments[i]['start'], segments[i]['end']))

    print(f"  åˆ é™¤é‡å¤å­—: {repeat_count} ä¸ª")
    print()

    # 4. ç”Ÿæˆåˆ é™¤åˆ—è¡¨
    print("ç”Ÿæˆåˆ é™¤åˆ—è¡¨...")
    to_delete = []

    # æ·»åŠ è¯­æ°”è¯åˆ é™¤
    buffer_ms = int(config.get('buffer', {}).get('before', 0.05) * 1000)
    for filler in potential_fillers:
        if filler['should_delete']:
            start = max(0, filler['start_ms'] - buffer_ms)
            end = filler['end_ms'] + buffer_ms
            to_delete.append((start, end))

    # æ·»åŠ é‡å¤å­—åˆ é™¤
    for start, end in repeat_deletions:
        to_delete.append((start, end))

    # å¯é€‰ï¼šé™éŸ³åˆ é™¤
    remove_silence = config.get('silence', {}).get('enable', False)
    if remove_silence:
        threshold = config.get('silence', {}).get('threshold', 1.0) * 1000
        if segments[0]['start'] > threshold:
            to_delete.append((0, segments[0]['start']))
        for i in range(len(segments) - 1):
            gap = segments[i+1]['start'] - segments[i]['end']
            if gap >= threshold:
                to_delete.append((segments[i]['end'], segments[i+1]['start']))

    if not to_delete:
        print("âŒ æœªæ£€æµ‹åˆ°éœ€è¦åˆ é™¤çš„ç‰‡æ®µ")
        return []

    # åˆå¹¶æ—¶é—´æ®µ
    to_delete.sort(key=lambda x: x[0])
    merged = []
    curr_s, curr_e = to_delete[0]
    for s, e in to_delete[1:]:
        if s <= curr_e + 150:  # 150mså†…åˆå¹¶
            curr_e = max(curr_e, e)
        else:
            merged.append((curr_s, curr_e))
            curr_s, curr_e = s, e
    merged.append((curr_s, curr_e))

    total_delete_time = sum(e - s for s, e in merged) / 1000.0
    print(f"åˆå¹¶ååˆ é™¤æ®µæ•°: {len(merged)}")
    print(f"æ€»åˆ é™¤æ—¶é•¿: {total_delete_time:.2f}ç§’")
    print()

    # è®¡ç®—ä¿ç•™æ®µ
    duration_ms = data['duration_ms']
    keeps = []
    curr_time = 0
    merged_sec = [(s/1000.0, e/1000.0) for s, e in merged]

    for s, e in merged_sec:
        if s > curr_time:
            keeps.append((curr_time, s))
        curr_time = max(curr_time, e)

    if curr_time < duration_ms/1000.0:
        keeps.append((curr_time, duration_ms/1000.0))

    print(f"ä¿ç•™æ®µæ•°: {len(keeps)}")
    print()

    # ç”ŸæˆFilterï¼ˆå¸¦éŸ³é¢‘äº¤å‰æ·¡åŒ–ï¼‰
    if not keeps:
        print("âŒ è­¦å‘Šï¼šæ‰€æœ‰å†…å®¹éƒ½è¢«åˆ é™¤äº†ï¼")
        return []

    filter_complex = ""
    inputs = ""
    fade_duration = 0.05  # 50ms

    for i, (start, end) in enumerate(keeps):
        filter_complex += f"[0:v]trim=start={start}:end={end},setpts=PTS-STARTPTS[v{i}];"

        if i == 0 and len(keeps) > 1:
            filter_complex += f"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS,afade=t=in:ss=0:d={fade_duration}[a{i}];"
        elif i == len(keeps) - 1 and len(keeps) > 1:
            clip_duration = end - start
            fade_start = clip_duration - fade_duration
            filter_complex += f"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS,afade=t=out:st={fade_start}:d={fade_duration}[a{i}];"
        elif len(keeps) > 1:
            clip_duration = end - start
            fade_start = clip_duration - fade_duration
            filter_complex += f"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS,afade=t=in:ss=0:d={fade_duration},afade=t=out:st={fade_start}:d={fade_duration}[a{i}];"
        else:
            filter_complex += f"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS[a{i}];"

        inputs += f"[v{i}][a{i}]"

    filter_complex += f"{inputs}concat=n={len(keeps)}:v=1:a=1[outv][outa]"

    with open(output_filter_file, 'w', encoding='utf-8') as f:
        f.write(filter_complex)

    print("=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ Filter: {output_filter_file}")
    print(f"ğŸµ éŸ³é¢‘æ·¡åŒ–: {fade_duration*1000:.0f}ms")
    print(f"ğŸ“Š é¢„è®¡ä¿ç•™: {duration_ms/1000 - total_delete_time:.1f}ç§’ / {duration_ms/1000:.1f}ç§’ ({(1 - total_delete_time/(duration_ms/1000))*100:.1f}%)")
    print()

    return keeps

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("transcript", help="è½¬å½•JSONæ–‡ä»¶")
    parser.add_argument("output", help="è¾“å‡ºFilteræ–‡ä»¶")
    parser.add_argument("--config", default="config.yaml", help="é…ç½®æ–‡ä»¶")
    parser.add_argument("--no-llm", action="store_true", help="ç¦ç”¨LLMåˆ†æ")
    args = parser.parse_args()

    analyze_transcript(args.transcript, args.output, args.config, not args.no_llm)
