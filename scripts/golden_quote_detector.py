#!/usr/bin/env python3
"""
é‡‘å¥æ£€æµ‹å™¨ - æ™ºèƒ½è¯†åˆ«è§†é¢‘ä¸­çš„ç²¾å½©ç‰‡æ®µ
æ”¯æŒå¤šç§è§„åˆ™ï¼šå…³é”®è¯ã€å¥å¼æ¨¡å¼ã€é•¿åº¦ã€AIåˆ†æ
"""

import json
import sys
import re
import argparse
from typing import List, Dict, Tuple
from dataclasses import dataclass
from collections import defaultdict

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


@dataclass
class Quote:
    """é‡‘å¥æ•°æ®ç»“æ„"""
    text: str
    start_ms: int
    end_ms: int
    score: float
    reason: str  # æ£€æµ‹åŸå› 
    timestamp: str  # æ ¼å¼åŒ–çš„æ—¶é—´æˆ³


class GoldenQuoteDetector:
    """é‡‘å¥æ£€æµ‹å™¨ä¸»ç±»"""

    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.quotes: List[Quote] = []

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        import yaml

        default_config = {
            'golden_quotes': {
                'enable': True,
                'rules': []
            }
        }

        if config_path:
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
            except Exception as e:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

        return default_config

    def detect(self, transcript_file: str, output_file: str = None) -> List[Quote]:
        """
        æ£€æµ‹é‡‘å¥

        Args:
            transcript_file: è½¬å½•JSONæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ£€æµ‹åˆ°çš„é‡‘å¥åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æ£€æµ‹é‡‘å¥...")

        # åŠ è½½è½¬å½•æ•°æ®
        with open(transcript_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        segments = data['segments']
        if not segments:
            print("âŒ è½¬å½•æ•°æ®ä¸ºç©º")
            return []

        # å°†å­—ç¬¦çº§ç‰‡æ®µè½¬æ¢ä¸ºå¥å­çº§
        sentences = self._segment_to_sentences(segments)

        # åº”ç”¨æ‰€æœ‰è§„åˆ™
        self.quotes = []
        quote_config = self.config.get('golden_quotes', {})

        for rule in quote_config.get('rules', []):
            rule_type = rule.get('type')

            if rule_type == 'keyword':
                self._detect_by_keywords(sentences, rule.get('keywords', []))

            elif rule_type == 'pattern':
                self._detect_by_patterns(sentences, rule.get('patterns', []))

            elif rule_type == 'length':
                self._detect_by_length(sentences, rule)

            elif rule_type == 'ai':
                if rule.get('enable', False):
                    self._detect_by_ai(sentences, rule)

        # å»é‡å’Œæ’åº
        self.quotes = self._deduplicate_quotes()
        self.quotes.sort(key=lambda x: x.score, reverse=True)

        # è¾“å‡ºç»“æœ
        if output_file:
            self._save_quotes(output_file, data.get('video_path', ''))

        self._print_summary()
        return self.quotes

    def _segment_to_sentences(self, segments: List[Dict]) -> List[Dict]:
        """å°†å­—ç¬¦çº§ç‰‡æ®µè½¬æ¢ä¸ºå¥å­çº§"""
        sentences = []
        current_sentence = []
        current_start = None

        for i, seg in enumerate(segments):
            if not current_sentence:
                current_start = seg['start']

            current_sentence.append(seg['char'])

            # å¥å­ç»“æŸæ ‡è®°ï¼šã€‚ï¼ï¼Ÿâ€¦â€¦\n
            if seg['char'] in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', 'â€¦', '\n']:
                text = ''.join(current_sentence).strip()
                if text:
                    sentences.append({
                        'text': text,
                        'start': current_start,
                        'end': seg['end']
                    })
                current_sentence = []
                current_start = None

            # å¤„ç†æ ‡ç‚¹åçš„åœé¡¿ï¼ˆè¶…è¿‡ 500ms è®¤ä¸ºæ˜¯æ–°å¥å­ï¼‰
            elif i < len(segments) - 1:
                gap = segments[i + 1]['start'] - seg['end']
                if gap > 500 and current_sentence:
                    text = ''.join(current_sentence).strip()
                    if text:
                        sentences.append({
                            'text': text,
                            'start': current_start,
                            'end': seg['end']
                        })
                    current_sentence = []
                    current_start = None

        # å¤„ç†æœ€åå‰©ä½™çš„å†…å®¹
        if current_sentence:
            text = ''.join(current_sentence).strip()
            if text:
                sentences.append({
                    'text': text,
                    'start': current_start,
                    'end': segments[-1]['end']
                })

        return sentences

    def _detect_by_keywords(self, sentences: List[Dict], keywords: List[str]):
        """åŸºäºå…³é”®è¯æ£€æµ‹"""
        print(f"  ğŸ“Œ å…³é”®è¯è§„åˆ™: {len(keywords)} ä¸ªå…³é”®è¯")

        for sent in sentences:
            text = sent['text']
            for keyword in keywords:
                if keyword in text:
                    # è®¡ç®—åˆ†æ•°ï¼šå…³é”®è¯å‡ºç°æ¬¡æ•° + å¥å­é•¿åº¦
                    count = text.count(keyword)
                    score = 10 * count + min(len(text) / 10, 10)

                    self.quotes.append(Quote(
                        text=text,
                        start_ms=sent['start'],
                        end_ms=sent['end'],
                        score=score,
                        reason=f"åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€",
                        timestamp=self._format_timestamp(sent['start'])
                    ))
                    break  # ä¸€ä¸ªå¥å­åªè®°å½•ä¸€æ¬¡

    def _detect_by_patterns(self, sentences: List[Dict], patterns: List[str]):
        """åŸºäºæ­£åˆ™æ¨¡å¼æ£€æµ‹"""
        print(f"  ğŸ”§ å¥å¼è§„åˆ™: {len(patterns)} ä¸ªæ¨¡å¼")

        compiled_patterns = [re.compile(p) for p in patterns]

        for sent in sentences:
            text = sent['text']
            for pattern in compiled_patterns:
                if pattern.search(text):
                    score = 15 + min(len(text) / 10, 10)

                    self.quotes.append(Quote(
                        text=text,
                        start_ms=sent['start'],
                        end_ms=sent['end'],
                        score=score,
                        reason=f"åŒ¹é…å¥å¼æ¨¡å¼",
                        timestamp=self._format_timestamp(sent['start'])
                    ))
                    break

    def _detect_by_length(self, sentences: List[Dict], rule: Dict):
        """åŸºäºé•¿åº¦å’Œå¤æ‚åº¦æ£€æµ‹"""
        min_chars = rule.get('min_chars', 15)
        max_chars = rule.get('max_chars', 100)
        min_words = rule.get('min_words', 5)

        print(f"  ğŸ“ é•¿åº¦è§„åˆ™: {min_chars}-{max_chars} å­—ï¼Œ{min_words}+ è¯")

        for sent in sentences:
            text = sent['text']
            char_count = len(text)
            word_count = len(text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').split())

            if min_chars <= char_count <= max_chars and word_count >= min_words:
                # é¢å¤–åŠ åˆ†ï¼šåŒ…å«æ ‡ç‚¹ã€æ•°å­—ç­‰
                bonus = 0
                if 'ï¼Œ' in text or 'ï¼š' in text:
                    bonus += 2
                if any(c.isdigit() for c in text):
                    bonus += 3

                score = 5 + bonus

                self.quotes.append(Quote(
                    text=text,
                    start_ms=sent['start'],
                    end_ms=sent['end'],
                    score=score,
                    reason=f"ä¼˜ç§€é•¿åº¦ ({char_count} å­—)",
                    timestamp=self._format_timestamp(sent['start'])
                ))

    def _detect_by_ai(self, sentences: List[Dict], rule: Dict):
        """ä½¿ç”¨ AI åˆ†ææ£€æµ‹é‡‘å¥ï¼ˆéœ€è¦ APIï¼‰"""
        print("  ğŸ¤– AI åˆ†æè§„åˆ™")

        try:
            import openai
            api_key = rule.get('api_key') or os.getenv('OPENAI_API_KEY')
            if not api_key:
                print("  âš ï¸ æœªé…ç½® API Keyï¼Œè·³è¿‡ AI åˆ†æ")
                return

            client = openai.OpenAI(api_key=api_key)
            model = rule.get('model', 'gpt-4')
            max_quotes = rule.get('max_quotes', 5)

            # æ„å»ºæç¤ºè¯
            all_text = '\n'.join([f"{i+1}. {s['text']}" for i, s in enumerate(sentences)])

            prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æ‰¾å‡º {max_quotes} æœ€æœ‰ä»·å€¼çš„é‡‘å¥ï¼ˆåè¨€ã€æ€»ç»“ã€é‡ç‚¹ã€ç²¾å½©è§‚ç‚¹ï¼‰ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{all_text}

è¯·åªè¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- index: é‡‘å¥åºå·ï¼ˆ1-{len(sentences)}ï¼‰
- reason: é€‰æ‹©ç†ç”±ï¼ˆç®€çŸ­ï¼‰
- score: è¯„åˆ†ï¼ˆ1-100ï¼‰

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
  {{"index": 5, "reason": "ç²¾è¾Ÿçš„æ€»ç»“", "score": 95}},
  {{"index": 12, "reason": "æ ¸å¿ƒè§‚ç‚¹", "score": 88}}
]
"""

            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )

            result = json.loads(response.choices[0].message.content)

            for item in result:
                idx = item['index'] - 1
                if 0 <= idx < len(sentences):
                    sent = sentences[idx]
                    self.quotes.append(Quote(
                        text=sent['text'],
                        start_ms=sent['start'],
                        end_ms=sent['end'],
                        score=item['score'],
                        reason=f"AIåˆ†æ: {item['reason']}",
                        timestamp=self._format_timestamp(sent['start'])
                    ))

            print(f"  âœ… AI åˆ†æå®Œæˆï¼Œè¯†åˆ« {len(result)} æ¡é‡‘å¥")

        except ImportError:
            print("  âš ï¸ æœªå®‰è£… openai åº“ï¼Œè·³è¿‡ AI åˆ†æ")
        except Exception as e:
            print(f"  âŒ AI åˆ†æå¤±è´¥: {e}")

    def _deduplicate_quotes(self) -> List[Quote]:
        """å»é‡ï¼šç§»é™¤é‡å çš„é‡‘å¥"""
        if not self.quotes:
            return []

        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        sorted_quotes = sorted(self.quotes, key=lambda x: x.start_ms)
        unique = []

        for quote in sorted_quotes:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿ç•™çš„é‡‘å¥é‡å 
            is_duplicate = False
            for kept in unique:
                # å¦‚æœé‡å è¶…è¿‡ 50%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                overlap_start = max(quote.start_ms, kept.start_ms)
                overlap_end = min(quote.end_ms, kept.end_ms)
                overlap_duration = overlap_end - overlap_start
                quote_duration = quote.end_ms - quote.start_ms

                if overlap_duration > quote_duration * 0.5:
                    is_duplicate = True
                    # ä¿ç•™åˆ†æ•°æ›´é«˜çš„
                    if quote.score > kept.score:
                        unique.remove(kept)
                        unique.append(quote)
                    break

            if not is_duplicate:
                unique.append(quote)

        return unique

    def _format_timestamp(self, ms: int) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        seconds = ms / 1000
        m = int(seconds // 60)
        s = int(seconds % 60)
        return f"{m:02d}:{s:02d}"

    def _save_quotes(self, output_file: str, video_path: str):
        """ä¿å­˜é‡‘å¥åˆ°æ–‡ä»¶"""
        data = {
            'video_path': video_path,
            'total_quotes': len(self.quotes),
            'quotes': [
                {
                    'text': q.text,
                    'start_ms': q.start_ms,
                    'end_ms': q.end_ms,
                    'score': q.score,
                    'reason': q.reason,
                    'timestamp': q.timestamp
                }
                for q in self.quotes
            ]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ é‡‘å¥å·²ä¿å­˜è‡³: {output_file}")

    def _print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        if not self.quotes:
            print("âŒ æœªæ£€æµ‹åˆ°é‡‘å¥")
            return

        print(f"\nâœ… æ£€æµ‹åˆ° {len(self.quotes)} æ¡é‡‘å¥\n")

        # æ˜¾ç¤ºå‰ 10 æ¡
        for i, quote in enumerate(self.quotes[:10], 1):
            print(f"{i}. [{quote.timestamp}] {quote.text[:50]}{'...' if len(quote.text) > 50 else ''}")
            print(f"   ğŸ’¯ è¯„åˆ†: {quote.score:.1f} | {quote.reason}\n")

        if len(self.quotes) > 10:
            print(f"... è¿˜æœ‰ {len(self.quotes) - 10} æ¡\n")


def main():
    parser = argparse.ArgumentParser(
        description="é‡‘å¥æ£€æµ‹å™¨ - æ™ºèƒ½è¯†åˆ«è§†é¢‘ä¸­çš„ç²¾å½©ç‰‡æ®µ"
    )
    parser.add_argument("transcript", help="è½¬å½•JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„", default="golden_quotes.json")
    parser.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„", default="config.yaml")
    parser.add_argument("--top", type=int, help="åªä¿ç•™å‰ N æ¡é‡‘å¥", default=None)

    args = parser.parse_args()

    # æ£€æµ‹é‡‘å¥
    detector = GoldenQuoteDetector(args.config)
    detector.detect(args.transcript, args.output)

    # å¯é€‰ï¼šåªä¿ç•™å‰ N æ¡
    if args.top and len(detector.quotes) > args.top:
        detector.quotes = detector.quotes[:args.top]
        print(f"\nğŸ” åªä¿ç•™å‰ {args.top} æ¡é‡‘å¥")


if __name__ == "__main__":
    main()
