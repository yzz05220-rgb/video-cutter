#!/usr/bin/env python3
"""
WhisperX å¢å¼ºè½¬å½•å™¨
- æ›´å¿«çš„è½¬å½•é€Ÿåº¦ï¼ˆ70x realtimeï¼‰
- æ›´ç²¾å‡†çš„è¯çº§æ—¶é—´æˆ³
- æ”¯æŒè¯´è¯äººåˆ†ç¦»ï¼ˆdiarizationï¼‰
"""

import os
import sys
import json
import argparse

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def transcribe_with_whisperX(
    video_path: str,
    output_json: str,
    model_size: str = "large-v3",
    compute_type: str = "float16",
    diarization: bool = False,
    batch_size: int = 16
):
    """
    ä½¿ç”¨ WhisperX è¿›è¡Œå¢å¼ºè½¬å½•

    Args:
        video_path: è¾“å…¥è§†é¢‘è·¯å¾„
        output_json: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
        model_size: æ¨¡å‹å¤§å° (tiny/base/small/medium/large-v2/large-v3)
        compute_type: è®¡ç®—ç±»å‹ (float16/float32/int8)
        diarization: æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    import whisperx

    print(f"ğŸ¬ å¼€å§‹ WhisperX è½¬å½•: {os.path.basename(video_path)}")
    print(f"   æ¨¡å‹: {model_size}")
    print(f"   è®¡ç®—: {compute_type}")
    print(f"   æ‰¹å¤„ç†: {batch_size}")
    if diarization:
        print(f"   è¯´è¯äººåˆ†ç¦»: å¯ç”¨")

    # 1. è½¬å½•
    print("\nâ³ æ­¥éª¤ 1/3: è½¬å½•ä¸­...")
    try:
        device = "cuda" if __import__('torch').cuda.is_available() else "cpu"
        print(f"   è®¾å¤‡: {device}")

        audio = whisperx.load_audio(video_path)

        model = whisperx.load_model(
            model_size,
            device=device,
            compute_type=compute_type,
            language="zh"  # ä¸­æ–‡
        )

        result = model.transcribe(
            audio,
            batch_size=batch_size,
            language="zh"
        )

        print(f"   âœ… è½¬å½•å®Œæˆï¼Œè¯†åˆ«äº† {len(result['segments'])} ä¸ªç‰‡æ®µ")

    except Exception as e:
        print(f"   âŒ è½¬å½•å¤±è´¥: {e}")
        return False

    # 2. å¯¹é½ï¼ˆè¯çº§æ—¶é—´æˆ³ï¼‰
    print("\nâ³ æ­¥éª¤ 2/3: å¯¹é½è¯çº§æ—¶é—´æˆ³...")
    try:
        model_a, metadata = whisperx.load_align_model(
            language_code="zh",
            device=device
        )

        result = whisperx.align(
            result["segments"],
            model_a,
            metadata,
            audio,
            device,
            return_char_alignments=False  # ä½¿ç”¨è¯çº§å¯¹é½
        )

        print(f"   âœ… å¯¹é½å®Œæˆ")

    except Exception as e:
        print(f"   âš ï¸ å¯¹é½å¤±è´¥: {e}")
        print(f"   å°†ä½¿ç”¨å¥å­çº§æ—¶é—´æˆ³")

    # 3. è¯´è¯äººåˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
    if diarization:
        print("\nâ³ æ­¥éª¤ 3/3: è¯´è¯äººåˆ†ç¦»...")
        try:
            diarize_model = whisperx.DiarizationPipeline(
                use_auth_token=False,  # ä¸ä½¿ç”¨ HF token
                device=device
            )

            result = whisperx.assign_word_speakers(
                diarize_model,
                result,
                audio
            )

            print(f"   âœ… è¯´è¯äººåˆ†ç¦»å®Œæˆ")

        except Exception as e:
            print(f"   âš ï¸ è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
            print(f"   ç»§ç»­ä¸ä½¿ç”¨è¯´è¯äººä¿¡æ¯")

    # 4. è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
    print("\nğŸ“¦ è½¬æ¢è¾“å‡ºæ ¼å¼...")

    # è·å–è§†é¢‘æ—¶é•¿
    try:
        import subprocess
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            video_path
        ]
        result_check = subprocess.run(cmd, capture_output=True, text=True, check=True)
        duration_ms = float(result_check.stdout.strip()) * 1000
    except:
        duration_ms = sum([seg['end'] - seg['start'] for seg in result['segments']])

    # å¦‚æœæœ‰è¯çº§æ—¶é—´æˆ³ï¼Œä½¿ç”¨è¯çº§ï¼›å¦åˆ™ä½¿ç”¨å¥å­çº§
    all_chars = []

    for seg in result["segments"]:
        if "words" in seg and len(seg["words"]) > 0:
            # ä½¿ç”¨è¯çº§æ—¶é—´æˆ³
            for word in seg["words"]:
                if "word" in word:
                    # æ¯ä¸ªå­—ç¬¦ä½¿ç”¨è¯çš„æ—¶é—´æˆ³
                    for char in word["word"]:
                        all_chars.append({
                            'char': char,
                            'start': round(word["start"] * 1000),
                            'end': round(word["end"] * 1000)
                        })
        else:
            # ä½¿ç”¨å¥å­çº§æ—¶é—´æˆ³
            for char in seg["text"]:
                all_chars.append({
                    'char': char,
                    'start': round(seg["start"] * 1000),
                    'end': round(seg["end"] * 1000)
                })

    # ä¿å­˜ç»“æœ
    output_data = {
        "video_path": video_path,
        "duration_ms": duration_ms,
        "model": f"WhisperX-{model_size}",
        "segments": all_chars,
        "speaker_segments": result.get("segments", [])  # åŒ…å«è¯´è¯äººä¿¡æ¯
    }

    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… WhisperX è½¬å½•å®Œæˆï¼")
    print(f"   è¯†åˆ«å­—ç¬¦æ•°: {len(all_chars)}")
    print(f"   è§†é¢‘æ—¶é•¿: {duration_ms/1000:.1f} ç§’")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_json}")

    # æ‰“å°è¯´è¯äººç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if diarization and "segments" in result:
        speakers = set()
        for seg in result["segments"]:
            if "speaker" in seg:
                speakers.add(seg["speaker"])
        if speakers:
            print(f"   è¯†åˆ«è¯´è¯äºº: {len(speakers)} ä¸ª - {', '.join(sorted(speakers))}")

    return True


def main():
    parser = argparse.ArgumentParser(
        description="WhisperX å¢å¼ºè½¬å½•å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # åŸºç¡€ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤ large-v3 æ¨¡å‹ï¼‰
  python transcriber_whisperX.py video.mp4 output.json

  # ä½¿ç”¨ medium æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰
  python transcriber_whisperX.py video.mp4 output.json --model medium

  # å¯ç”¨è¯´è¯äººåˆ†ç¦»
  python transcriber_whisperX.py video.mp4 output.json --diarization

  # GPU åŠ é€Ÿï¼ˆfloat16ï¼‰
  python transcriber_whisperX.py video.mp4 output.json --compute float16

  # è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å‡å°ï¼‰
  python transcriber_whisperX.py video.mp4 output.json --batch-size 8
        """
    )

    parser.add_argument("video", help="è¾“å…¥è§†é¢‘è·¯å¾„")
    parser.add_argument("output", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="large-v3",
                       help="WhisperX æ¨¡å‹ (tiny/base/small/medium/large-v2/large-v3)")
    parser.add_argument("--compute", default="float16",
                       help="è®¡ç®—ç±»å‹ (float16/float32/int8)")
    parser.add_argument("--diarization", action="store_true",
                       help="å¯ç”¨è¯´è¯äººåˆ†ç¦»")
    parser.add_argument("--batch-size", type=int, default=16,
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤ 16ï¼Œæ˜¾å­˜ä¸è¶³æ—¶å¯å‡å°)")

    args = parser.parse_args()

    transcribe_with_whisperX(
        args.video,
        args.output,
        model_size=args.model,
        compute_type=args.compute,
        diarization=args.diarization,
        batch_size=args.batch_size
    )


if __name__ == "__main__":
    main()
