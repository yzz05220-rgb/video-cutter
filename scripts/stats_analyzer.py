#!/usr/bin/env python3
"""
ç»Ÿè®¡åˆ†æå·¥å…· - ç”Ÿæˆè¯¦ç»†çš„è§†é¢‘å‰ªè¾‘æŠ¥å‘Š
åŒ…æ‹¬æ—¶é•¿å¯¹æ¯”ã€è¯­é€Ÿåˆ†æã€åœé¡¿ç»Ÿè®¡ã€é‡‘å¥æ€»ç»“ç­‰
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List
from collections import Counter

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8ï¼ˆä»…åœ¨ç›´æ¥è¿è¡Œæ—¶ï¼‰
if sys.platform == 'win32' and __name__ == '__main__':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


class StatsAnalyzer:
    """ç»Ÿè®¡åˆ†æå™¨"""

    def __init__(self):
        self.stats = {}

    def generate_report(
        self,
        original_video: str,
        output_video: str = None,
        transcript_file: str = None,
        quotes_file: str = None,
        output_json: str = None
    ):
        """
        ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š

        Args:
            original_video: åŸè§†é¢‘è·¯å¾„
            output_video: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            transcript_file: è½¬å½•æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            quotes_file: é‡‘å¥æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            output_json: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
        """
        print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

        # 1. åŸºæœ¬ä¿¡æ¯
        self.stats['original_video'] = original_video
        self.stats['original_size_mb'] = self._get_file_size_mb(original_video)
        self.stats['original_duration'] = self._get_video_duration(original_video)

        if output_video and os.path.exists(output_video):
            self.stats['output_video'] = output_video
            self.stats['output_size_mb'] = self._get_file_size_mb(output_video)
            self.stats['output_duration'] = self._get_video_duration(output_video)
            self.stats['size_reduction'] = (1 - self.stats['output_size_mb'] / self.stats['original_size_mb']) * 100
            self.stats['duration_reduction'] = (1 - self.stats['output_duration'] / self.stats['original_duration']) * 100

        # 2. è½¬å½•åˆ†æ
        if transcript_file and os.path.exists(transcript_file):
            self._analyze_transcript(transcript_file)

        # 3. é‡‘å¥åˆ†æ
        if quotes_file and os.path.exists(quotes_file):
            self._analyze_quotes(quotes_file)

        # 4. æ‰“å°æŠ¥å‘Š
        self._print_report()

        # 5. ä¿å­˜ JSON
        if output_json:
            self._save_json(output_json)

    def _get_file_size_mb(self, file_path: str) -> float:
        """è·å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
        return os.path.getsize(file_path) / (1024 * 1024)

    def _get_video_duration(self, video_path: str) -> float:
        """è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
        import subprocess

        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            video_path
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return float(result.stdout.strip())
        except:
            return 0.0

    def _analyze_transcript(self, transcript_file: str):
        """åˆ†æè½¬å½•æ–‡ä»¶"""
        with open(transcript_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        segments = data['segments']
        duration_sec = data['duration_ms'] / 1000.0

        # æ–‡æœ¬ç»Ÿè®¡
        full_text = ''.join([s['char'] for s in segments])
        self.stats['total_chars'] = len(full_text)
        self.stats['total_words'] = len(full_text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').split())

        # è¯­é€Ÿåˆ†æï¼ˆå­—ç¬¦/åˆ†é’Ÿï¼‰
        self.stats['speech_rate_chars_per_min'] = (self.stats['total_chars'] / duration_sec) * 60
        self.stats['speech_rate_words_per_min'] = (self.stats['total_words'] / duration_sec) * 60

        # åœé¡¿åˆ†æ
        pauses = []
        for i in range(len(segments) - 1):
            gap = segments[i + 1]['start'] - segments[i]['end']
            if gap > 300:  # è¶…è¿‡ 300ms è®¤ä¸ºæ˜¯åœé¡¿
                pauses.append(gap / 1000.0)  # è½¬æ¢ä¸ºç§’

        self.stats['total_pauses'] = len(pauses)
        self.stats['pause_rate'] = len(pauses) / duration_sec * 60  # æ¯åˆ†é’Ÿåœé¡¿æ¬¡æ•°
        self.stats['avg_pause_duration'] = sum(pauses) / len(pauses) if pauses else 0
        self.stats['max_pause_duration'] = max(pauses) if pauses else 0

        # å­—ç¬¦é¢‘ç‡
        char_freq = Counter([s['char'] for s in segments if s['char'].strip()])
        self.stats['top_chars'] = char_freq.most_common(10)

        # å¡«å……è¯æ£€æµ‹
        filler_words = ['å—¯', 'å•Š', 'å“', 'è¯¶', 'å‘ƒ', 'é¢', 'å”‰', 'å“¦', 'å™¢', 'å‘€', 'æ¬¸', 'é‚£ä¸ª', 'ç„¶å', 'å°±æ˜¯']
        filler_count = sum([full_text.count(fw) for fw in filler_words])
        self.stats['filler_ratio'] = (filler_count / self.stats['total_chars'] * 100) if self.stats['total_chars'] > 0 else 0

    def _analyze_quotes(self, quotes_file: str):
        """åˆ†æé‡‘å¥æ–‡ä»¶"""
        with open(quotes_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        quotes = data.get('quotes', [])
        self.stats['total_quotes'] = len(quotes)
        self.stats['avg_quote_score'] = sum([q['score'] for q in quotes]) / len(quotes) if quotes else 0
        self.stats['top_quotes'] = quotes[:5] if quotes else []

    def _print_report(self):
        """æ‰“å°æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š è§†é¢‘å‰ªè¾‘ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 70)

        # æ—¶é•¿å¯¹æ¯”
        print("\nâ±ï¸  æ—¶é•¿å¯¹æ¯”:")
        orig_min = int(self.stats['original_duration'] // 60)
        orig_sec = int(self.stats['original_duration'] % 60)
        print(f"  åŸè§†é¢‘: {orig_min}åˆ†{orig_sec}ç§’ ({self.stats['original_duration']:.1f}ç§’)")

        if 'output_duration' in self.stats:
            out_min = int(self.stats['output_duration'] // 60)
            out_sec = int(self.stats['output_duration'] % 60)
            print(f"  å‰ªè¾‘å: {out_min}åˆ†{out_sec}ç§’ ({self.stats['output_duration']:.1f}ç§’)")
            print(f"  å‹ç¼©ç‡: {self.stats['duration_reduction']:.1f}%")

        # æ–‡ä»¶å¤§å°
        print("\nğŸ’¾ æ–‡ä»¶å¤§å°:")
        print(f"  åŸè§†é¢‘: {self.stats['original_size_mb']:.1f} MB")

        if 'output_size_mb' in self.stats:
            print(f"  å‰ªè¾‘å: {self.stats['output_size_mb']:.1f} MB")
            print(f"  å‡å°: {self.stats['size_reduction']:.1f}%")

        # è¯­é€Ÿåˆ†æ
        if 'speech_rate_chars_per_min' in self.stats:
            print("\nğŸ—£ï¸  è¯­é€Ÿåˆ†æ:")
            print(f"  å¹³å‡è¯­é€Ÿ: {self.stats['speech_rate_chars_per_min']:.0f} å­—/åˆ†é’Ÿ")
            print(f"           {self.stats['speech_rate_words_per_min']:.0f} è¯/åˆ†é’Ÿ")

            # é€Ÿåº¦è¯„çº§
            rate = self.stats['speech_rate_chars_per_min']
            if rate < 150:
                speed_label = "è¾ƒæ…¢"
            elif rate < 250:
                speed_label = "é€‚ä¸­"
            elif rate < 350:
                speed_label = "è¾ƒå¿«"
            else:
                speed_label = "æå¿«"
            print(f"  é€Ÿåº¦è¯„çº§: {speed_label}")

        # åœé¡¿ç»Ÿè®¡
        if 'total_pauses' in self.stats:
            print("\nâ¸ï¸  åœé¡¿ç»Ÿè®¡:")
            print(f"  åœé¡¿æ¬¡æ•°: {self.stats['total_pauses']} æ¬¡")
            print(f"  åœé¡¿é¢‘ç‡: {self.stats['pause_rate']:.1f} æ¬¡/åˆ†é’Ÿ")
            print(f"  å¹³å‡æ—¶é•¿: {self.stats['avg_pause_duration']:.2f} ç§’")
            print(f"  æœ€é•¿åœé¡¿: {self.stats['max_pause_duration']:.2f} ç§’")

        # å¡«å……è¯
        if 'filler_ratio' in self.stats:
            print("\nğŸ”¤ å¡«å……è¯:")
            print(f"  æ¯”ä¾‹: {self.stats['filler_ratio']:.1f}%")

        # é‡‘å¥
        if 'total_quotes' in self.stats:
            print(f"\nâœ¨ é‡‘å¥:")
            print(f"  æ£€æµ‹åˆ° {self.stats['total_quotes']} æ¡é‡‘å¥")
            print(f"  å¹³å‡è¯„åˆ†: {self.stats['avg_quote_score']:.1f}")

            if self.stats['top_quotes']:
                print(f"\n  ğŸ† Top 5 é‡‘å¥:")
                for i, q in enumerate(self.stats['top_quotes'], 1):
                    text = q['text'][:50] + '...' if len(q['text']) > 50 else q['text']
                    print(f"    {i}. [{q['timestamp']}] {text}")
                    print(f"       ğŸ’¯ {q['score']:.1f} | {q['reason']}")

        # å¸¸ç”¨å­—
        if 'top_chars' in self.stats:
            print(f"\nğŸ“ å¸¸ç”¨å­— Top 10:")
            for i, (char, count) in enumerate(self.stats['top_chars'], 1):
                if char.strip():
                    print(f"    {i}. ã€Œ{char}ã€: {count} æ¬¡")

        print("\n" + "=" * 70 + "\n")

    def _save_json(self, output_file: str):
        """ä¿å­˜ JSON æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {output_file}\n")


def main():
    parser = argparse.ArgumentParser(
        description="ç»Ÿè®¡åˆ†æå·¥å…· - ç”Ÿæˆè§†é¢‘å‰ªè¾‘ç»Ÿè®¡æŠ¥å‘Š"
    )

    parser.add_argument("--original", required=True, help="åŸè§†é¢‘è·¯å¾„")
    parser.add_argument("--output", help="å‰ªè¾‘åè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--transcript", help="è½¬å½•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--quotes", help="é‡‘å¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--report", help="è¾“å‡º JSON æŠ¥å‘Šè·¯å¾„", default="stats_report.json")

    args = parser.parse_args()

    analyzer = StatsAnalyzer()
    analyzer.generate_report(
        args.original,
        args.output,
        args.transcript,
        args.quotes,
        args.report
    )


if __name__ == "__main__":
    main()
